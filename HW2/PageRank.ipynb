{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2 PageRank\n",
    "106062314 蔡政諺"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper1\n",
    "將 input file 根據 ```\"\\n\"``` 切成一行一行，然後把每行轉換成 ```(i, [j])``` 與 ```(i, [])``` 兩個 key-value pair。<br>\n",
    "其中 i 是 source node ， j 是 destination node 。<br>\n",
    "Ex. ```\"0\t1\"``` → ```[(0, [1]), (0, [])]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper1(lines):\n",
    "    lines = lines.split(\"\\n\")\n",
    "    edges = []\n",
    "    for line in lines:\n",
    "        items = line.split(\"\\t\")\n",
    "        edges.append((int(items[0]), [int(items[1])]))\n",
    "        edges.append((int(items[1]), [])) # prevent nodes that have no out-degree disappear\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper2\n",
    "根據 node 的總數，給每個 node 賦初始 r 值。<br>\n",
    "Ex. ```x[0] = 2, n = 5``` → ```(2, 0.2)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper2(x, n):\n",
    "    return (x[0], 1/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper3\n",
    "對 source node 的 r 值進行分配。分為 2 個部分：<br>\n",
    "(1) $\\beta$<br>\n",
    "如果 source node 有 out degree 的話，這部分會均分給所有的 destination node 。<br>\n",
    "如果是dead end的話，這部分會均分給所有 node 。<br>\n",
    "Ex1. ```(1, ([2, 3], 0.2))``` → ```[(2, 0.08), (3, 0.08)]```<br>\n",
    "(2) $1-\\beta$<br>\n",
    "均分給所有的 node 。<br><br>\n",
    "所有需要均分給所有 node 的部分都透過後面的 renormalize 處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper3(x, n):\n",
    "    beta = 0.8\n",
    "    maplist = []\n",
    "    maplist.append((x[0], 0)) # prevent nodes that have no in-degree disappear\n",
    "    for node in x[1][0]:\n",
    "        val = beta*x[1][1]/len(x[1][0])\n",
    "        maplist.append((node, val))\n",
    "    return maplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper4\n",
    "進行 renormalization 。 S 是目前算出的 r 值的總和，因此必須對每個 node 都加上 (1-S)/n ，總和才會為 1 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper4(x, S, n):\n",
    "    return (x[0], x[1] + (1-S)/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer1\n",
    "這個 reducer 有兩個用途：<br>\n",
    "(1) 將同一個 source node 的 destination node(s) 合併成一個 list 。<br>\n",
    "Ex1. ```(0, [1]), (0, [2])``` → ```(0, [1, 2])```<br>\n",
    "Ex2. ```[(0, [1]), (0, []), (0, [2]), (0, []), (0, [3]), (0, [])]``` → ```(0, [1, 2, 3])```<br>\n",
    "(2) 將分配給同一個 node 的 r 值加總。<br>\n",
    "Ex. ```[(2, 0.08), (2, 0.032), (2, 0.008), (2, 0.008), (2, 0.008), (2, 0.008), (2, 0.008)]``` → ```(2, 0.152)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer1(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function\n",
    "(1) 將檔案 ```p2p-Gnutella04.txt``` 讀取，經過 ```mapper1``` 與 ```reducer1``` ，得到 ```edges``` ，也就是所有 source node 與 destination node 的相連關係。<br>\n",
    "(2) 使用 ```count()``` 將 node 的總數存進 ```n``` 。<br>\n",
    "(3) ```edges``` 經過 ```mapper2``` ，得到 ```r``` ，也就是所有 node 初始的 r 值。<br>\n",
    "(4) 將 ```edges``` 與 ```r``` join 起來，經過 ```mapper3``` 與 ```reducer1``` 。<br>\n",
    "(5) 使用 ```sum()``` 算出當前 r 值的總和，再經過 ```mapper4``` 做 renormalization ，即可得到總和為 1 的 r 值。(4) 與 (5) 共執行 20 次。<br>\n",
    "(6) 使用 ```sortBy()``` 根據 r 值對所有 node 排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges: [(0, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), (1, [2, 11, 12, 13, 14, 15, 16, 17, 18, 19]), (2, []), (3, [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]), (4, []), (5, []), (6, []), (7, []), (8, [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]), (9, [])]\n",
      "# of nodes: 10876\n",
      "init: [(0, 9.194556822361162e-05), (1, 9.194556822361162e-05), (2, 9.194556822361162e-05), (3, 9.194556822361162e-05), (4, 9.194556822361162e-05), (5, 9.194556822361162e-05), (6, 9.194556822361162e-05), (7, 9.194556822361162e-05), (8, 9.194556822361162e-05), (9, 9.194556822361162e-05)]\n",
      "iter 1 / 20\n",
      "iter 2 / 20\n",
      "iter 3 / 20\n",
      "iter 4 / 20\n",
      "iter 5 / 20\n",
      "iter 6 / 20\n",
      "iter 7 / 20\n",
      "iter 8 / 20\n",
      "iter 9 / 20\n",
      "iter 10 / 20\n",
      "iter 11 / 20\n",
      "iter 12 / 20\n",
      "iter 13 / 20\n",
      "iter 14 / 20\n",
      "iter 15 / 20\n",
      "iter 16 / 20\n",
      "iter 17 / 20\n",
      "iter 18 / 20\n",
      "iter 19 / 20\n",
      "iter 20 / 20\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "filename = \"p2p-Gnutella04.txt\"\n",
    "sc.stop()\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"pagerank\")\n",
    "sc = SparkContext(conf=conf)\n",
    "edges = sc.textFile(filename).flatMap(mapper1)\n",
    "edges = edges.reduceByKey(reducer1)\n",
    "print(\"edges:\", edges.take(10))\n",
    "n = edges.count()\n",
    "print(\"# of nodes:\", n)\n",
    "r = edges.map(lambda x: mapper2(x, n))\n",
    "print(\"init:\", r.take(10))\n",
    "\n",
    "iterations = 20\n",
    "for i in range(1, iterations+1):\n",
    "    pairs = edges.join(r)\n",
    "    pairs = pairs.flatMap(lambda x: mapper3(x, n))\n",
    "    r = pairs.reduceByKey(reducer1)\n",
    "    S = r.map(lambda x: x[1]).sum()\n",
    "    r = r.map(lambda x: mapper4(x, S, n))\n",
    "    print(\"iter\", i, \"/\", iterations)\n",
    "    # print(r.take(10))\n",
    "    \n",
    "r = r.sortBy(lambda x: -x[1])\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the result\n",
    "將 r 值最大的 10 個 node 依序輸出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1056, 0.0006321988095901941), (1054, 0.0006291557128603987), (1536, 0.0005239103397527083), (171, 0.0005116224706020384), (453, 0.0004956586476699697), (407, 0.0004848441996390269), (263, 0.000479619289318484), (4664, 0.0004704975514074376), (261, 0.00046289158656890185), (410, 0.0004615100382904272)]\n"
     ]
    }
   ],
   "source": [
    "out = r.take(10)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./Outputfile.txt\", 'w')\n",
    "for pair in out:\n",
    "    f.write(\"%d\t%.60f\\n\" % (pair[0], pair[1]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
